# Training Configuration
# Modify these settings to customize your training

[model]
# Base model to fine-tune
# Options: codellama/CodeLlama-7b-hf, mistralai/Mistral-7B-v0.1, meta-llama/Llama-2-7b-hf
base_model = codellama/CodeLlama-7b-hf

# Use LoRA for parameter-efficient training
use_lora = true

# Use 4-bit quantization (QLoRA) - reduces VRAM usage
use_4bit = true

[lora]
# LoRA rank - higher = more parameters (8, 16, 32, 64)
rank = 16

# LoRA alpha - scaling factor (typically 2x rank)
alpha = 32

# LoRA dropout
dropout = 0.05

[training]
# Number of training epochs
sft_epochs = 3
dpo_epochs = 1

# Batch sizes (reduce if OOM)
train_batch_size = 4
eval_batch_size = 4

# Gradient accumulation (effective batch = batch_size * accumulation)
gradient_accumulation_steps = 4

# Learning rates
sft_learning_rate = 2e-4
dpo_learning_rate = 5e-5

# Maximum sequence length
max_length = 2048

# Mixed precision training
use_bf16 = true
use_fp16 = false

# Gradient checkpointing (saves VRAM, slightly slower)
gradient_checkpointing = true

[dpo]
# DPO beta parameter (0.1-0.5, higher = more conservative)
beta = 0.1

# DPO loss type (sigmoid, hinge, ipo)
loss_type = sigmoid

# Number of negative examples per positive
num_negatives = 2

[data]
# Train/validation split ratio
train_split = 0.9

# Maximum prompt length for DPO
max_prompt_length = 512

[logging]
# Logging frequency
logging_steps = 10

# Evaluation frequency
eval_steps = 100

# Save checkpoint frequency
save_steps = 100

# Maximum checkpoints to keep
save_total_limit = 3

# Reporting (wandb, tensorboard, none)
report_to = wandb

[output]
# Output directories
sft_output_dir = ./outputs/sft_model
dpo_output_dir = ./outputs/dpo_model
data_output_dir = ./processed_data

[hardware]
# Device (auto, cuda, cpu)
device = auto

# Number of GPUs to use (for multi-GPU training)
num_gpus = 1

# DeepSpeed config (optional, for large models)
use_deepspeed = false
deepspeed_config = ./deepspeed_config.json
